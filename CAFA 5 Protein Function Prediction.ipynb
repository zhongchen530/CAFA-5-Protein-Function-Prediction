{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "29598d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio import SeqIO\n",
    "import statistics\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import os\n",
    "from collections import defaultdict\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9170300",
   "metadata": {},
   "outputs": [],
   "source": [
    "#graph = obonet.read_obo(\"/content/drive/MyDrive/cafa-5-protein-function-prediction/Train/go-basic.obo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "38e78fc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sequence</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EntryID</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>P20536</th>\n",
       "      <td>MNSVTVSHAPYTITYHDDWEPVMSQLVEFYNEVASWLLRDETSPIP...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>O73864</th>\n",
       "      <td>MTEYRNFLLLFITSLSVIYPCTGISWLGLTINGSSVGWNQTHHCKL...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>O95231</th>\n",
       "      <td>MRLSSSPPRGPQQLSSFGSVDWLSQSSCSGPTHTPRPADFSLGSLP...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A0A0B4J1F4</th>\n",
       "      <td>MGGEAGADGPRGRVKSLGLVFEDESKGCYSSGETVAGHVLLEAAEP...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P54366</th>\n",
       "      <td>MVETNSPPAGYTLKRSPSDLGEQQQPPRQISRSPGNTAAYHLTTAM...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     Sequence\n",
       "EntryID                                                      \n",
       "P20536      MNSVTVSHAPYTITYHDDWEPVMSQLVEFYNEVASWLLRDETSPIP...\n",
       "O73864      MTEYRNFLLLFITSLSVIYPCTGISWLGLTINGSSVGWNQTHHCKL...\n",
       "O95231      MRLSSSPPRGPQQLSSFGSVDWLSQSSCSGPTHTPRPADFSLGSLP...\n",
       "A0A0B4J1F4  MGGEAGADGPRGRVKSLGLVFEDESKGCYSSGETVAGHVLLEAAEP...\n",
       "P54366      MVETNSPPAGYTLKRSPSDLGEQQQPPRQISRSPGNTAAYHLTTAM..."
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.chdir(\"/Users/zhongyuanchen\")\n",
    "fasta_sequences = SeqIO.parse(open(\"Train/train_sequences.fasta\"),'fasta')\n",
    "df = pd.DataFrame((fasta.id,str(fasta.seq)) for fasta in fasta_sequences)\n",
    "df.columns = [\"EntryID\",\"Sequence\"]\n",
    "df.set_index(\"EntryID\",inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "062bdbc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trim or pad each sequence to a specific size TRIM_LENGTH\n",
    "TRIM_LENGTH = 400\n",
    "df[\"Sequence\"] = df[\"Sequence\"].apply(lambda x:x[:TRIM_LENGTH] if len(x)>TRIM_LENGTH else x.ljust(TRIM_LENGTH,\"0\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "01fde318",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'A': 0, 'B': 1, 'C': 2, 'D': 3, 'E': 4, 'F': 5, 'G': 6, 'H': 7, 'I': 8, 'J': 9, 'K': 10, 'L': 11, 'M': 12, 'N': 13, 'O': 14, 'P': 15, 'Q': 16, 'R': 17, 'S': 18, 'T': 19, 'U': 20, 'V': 21, 'W': 22, 'X': 23, 'Y': 24, 'Z': 25, '0': 26}\n"
     ]
    }
   ],
   "source": [
    "# Define a function to one hot encode a given fasta sequence, the resulting array will have size (TRIM_LENGTH,27)\n",
    "def sq_one_hot(sequence):\n",
    "    mapping = sq_one_hot.mapping\n",
    "    seq = [mapping[char] for char in sequence]\n",
    "    return np.eye(len(mapping))[seq].reshape(TRIM_LENGTH,27,1)\n",
    "\n",
    "sq_one_hot.mapping = {chr(i + ord(\"A\")):i for i in range(26)}\n",
    "sq_one_hot.mapping[\"0\"] = 26\n",
    "print(sq_one_hot.mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6d4f55a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the GO-terms data\n",
    "terms_df = pd.read_csv('Train/train_terms.tsv',sep=\"\\t\")\n",
    "terms_df.drop(columns = [\"aspect\"],inplace = True)\n",
    "terms_df.set_index(\"EntryID\",inplace = True)\n",
    "freq_counts = terms_df[\"term\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "07a07802",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.824170378699083"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select num_labels most frequent GO terms and restrict to these prediction\n",
    "num_labels = 1500\n",
    "chosen_terms = freq_counts.index[:num_labels]\n",
    "chosent_terms = set(chosen_terms)\n",
    "filt = terms_df[\"term\"].isin(chosen_terms)\n",
    "terms_df = terms_df[filt]\n",
    "sum(freq_counts.iloc[:num_labels])/sum(freq_counts.iloc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c0c8e4c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MultiLabelBinarizer()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MultiLabelBinarizer</label><div class=\"sk-toggleable__content\"><pre>MultiLabelBinarizer()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MultiLabelBinarizer()"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Multilabel encoding\n",
    "terms_mlb = MultiLabelBinarizer()\n",
    "terms_mlb.fit([terms_df['term']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ff5d1bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define dictionaries for fast acessing of terms and sequences using their EntryIDs\n",
    "id_sq = {entry_id:df.loc[entry_id][\"Sequence\"] for entry_id in df.index}\n",
    "id_terms = defaultdict(list)\n",
    "for entry_id,row in terms_df.iterrows():\n",
    "    id_terms[entry_id].append(row[\"term\"])\n",
    "\n",
    "\n",
    "# Define a data generator object to generate data for each epoch\n",
    "# Without generator, the size of all sequences add up to 30 gigabytes\n",
    "class Data_Generator(tf.keras.utils.Sequence):\n",
    "\n",
    "    def __init__(self,indexs, batch_size):\n",
    "        self.indexs = indexs\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return math.floor(len(self.indexs) / self.batch_size)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        low = idx * self.batch_size\n",
    "        # Cap upper bound at array length; the last batch may be smaller\n",
    "        # if the total number of items is not a multiple of batch size.\n",
    "        high = min(low + self.batch_size, len(self.indexs))\n",
    "        batch_x = []\n",
    "        batch_y = []\n",
    "\n",
    "        for i in range(low,high):\n",
    "            entry_id = self.indexs[i]\n",
    "            anotation = id_terms[entry_id]\n",
    "            batch_x.append(sq_one_hot(id_sq[entry_id]))\n",
    "            batch_y.append(terms_mlb.transform([anotation])[0])\n",
    "\n",
    "        return np.array(batch_x).reshape(self.batch_size,TRIM_LENGTH,27,1), np.array(batch_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2fd76f57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "83/83 [==============================] - 142s 2s/step - loss: 0.1070 - binary_accuracy: 0.9742 - auc_7: 0.7736 - val_loss: 0.0774 - val_binary_accuracy: 0.9800 - val_auc_7: 0.8380\n",
      "Epoch 2/10\n",
      "83/83 [==============================] - 130s 2s/step - loss: 0.0769 - binary_accuracy: 0.9800 - auc_7: 0.8409 - val_loss: 0.0770 - val_binary_accuracy: 0.9800 - val_auc_7: 0.8405\n",
      "Epoch 3/10\n",
      "83/83 [==============================] - 129s 2s/step - loss: 0.0760 - binary_accuracy: 0.9801 - auc_7: 0.8471 - val_loss: 0.0761 - val_binary_accuracy: 0.9800 - val_auc_7: 0.8464\n",
      "Epoch 4/10\n",
      "83/83 [==============================] - 130s 2s/step - loss: 0.0746 - binary_accuracy: 0.9802 - auc_7: 0.8577 - val_loss: 0.0753 - val_binary_accuracy: 0.9800 - val_auc_7: 0.8530\n",
      "Epoch 5/10\n",
      "83/83 [==============================] - 131s 2s/step - loss: 0.0732 - binary_accuracy: 0.9802 - auc_7: 0.8691 - val_loss: 0.0746 - val_binary_accuracy: 0.9800 - val_auc_7: 0.8594\n",
      "Epoch 6/10\n",
      "83/83 [==============================] - 129s 2s/step - loss: 0.0714 - binary_accuracy: 0.9804 - auc_7: 0.8817 - val_loss: 0.0740 - val_binary_accuracy: 0.9801 - val_auc_7: 0.8630\n",
      "Epoch 7/10\n",
      "83/83 [==============================] - 129s 2s/step - loss: 0.0688 - binary_accuracy: 0.9806 - auc_7: 0.8982 - val_loss: 0.0734 - val_binary_accuracy: 0.9801 - val_auc_7: 0.8666\n",
      "Epoch 8/10\n",
      "83/83 [==============================] - 132s 2s/step - loss: 0.0649 - binary_accuracy: 0.9809 - auc_7: 0.9187 - val_loss: 0.0734 - val_binary_accuracy: 0.9801 - val_auc_7: 0.8652\n",
      "Epoch 9/10\n",
      "83/83 [==============================] - 129s 2s/step - loss: 0.0594 - binary_accuracy: 0.9815 - auc_7: 0.9408 - val_loss: 0.0740 - val_binary_accuracy: 0.9800 - val_auc_7: 0.8597\n",
      "Epoch 10/10\n",
      "83/83 [==============================] - 129s 2s/step - loss: 0.0527 - binary_accuracy: 0.9824 - auc_7: 0.9612 - val_loss: 0.0757 - val_binary_accuracy: 0.9799 - val_auc_7: 0.8508\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2dbea0610>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create training and testing datas by shuffling and spliting an array of EntryIDs\n",
    "indexs = list(df.index)\n",
    "random.shuffle(indexs)\n",
    "train_test_split = int(0.3*len(indexs))\n",
    "train_indexs = indexs[:train_test_split]\n",
    "test_indexs = indexs[train_test_split:]\n",
    "\n",
    "# Define the respected generators for training and testing\n",
    "train_generator = Data_Generator(train_indexs,512)\n",
    "test_generator = Data_Generator(test_indexs,512)\n",
    "\n",
    "# Create a simple CNN for multilabel classification\n",
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(32, (8, 27), activation='relu', input_shape=(TRIM_LENGTH, 27, 1)))\n",
    "#model.add(layers.MaxPooling2D(pool_size=(2,1), strides=None, padding=\"valid\"))\n",
    "#model.add(layers.Conv2D(32,(32,1),activation='relu'))\n",
    "#model.add(layers.MaxPooling2D(pool_size=(6,1), strides=None, padding=\"valid\"))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(num_labels,activation = 'sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam',metrics=['binary_accuracy', tf.keras.metrics.AUC()])\n",
    "model.fit(x = train_generator,validation_data = test_generator,epochs = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac680bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in test data and preprocess similarly\n",
    "os.chdir(\"/Users/zhongyuanchen/Downloads/cafa-5-protein-function-prediction/Test (Targets)\")\n",
    "test_fasta_sq = SeqIO.parse(open(\"testsuperset.fasta\"),'fasta')\n",
    "test_df = pd.DataFrame((fasta.id,str(fasta.seq)) for fasta in test_fasta_sq)\n",
    "test_df.columns = [\"EntryID\",\"Sequence\"]\n",
    "\n",
    "# Trime each fasta sequence to have a TRIM_LENGTH size\n",
    "test_df[\"Sequence\"] = test_df[\"Sequence\"].apply(lambda x:x[:TRIM_LENGTH] if len(x)>TRIM_LENGTH else x.ljust(TRIM_LENGTH,\"0\"))\n",
    "test_df.set_index(\"EntryID\",inplace=True)\n",
    "\n",
    "# Similarly define generators for testing data\n",
    "test_id_sq = {}\n",
    "for entry_id,row in test_df.iterrows():\n",
    "    test_id_sq[entry_id] = row[\"Sequence\"]\n",
    "\n",
    "# Define generator for testing\n",
    "class Test_Data_Generator(tf.keras.utils.Sequence):\n",
    "\n",
    "    def __init__(self,indexs, batch_size):\n",
    "        self.indexs = indexs\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return math.floor(len(self.indexs) / self.batch_size)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        low = idx * self.batch_size\n",
    "        # Cap upper bound at array length; the last batch may be smaller\n",
    "        # if the total number of items is not a multiple of batch size.\n",
    "        high = min(low + self.batch_size, len(self.indexs))\n",
    "        batch_x = []\n",
    "\n",
    "        for i in range(low,high):\n",
    "            entry_id = self.indexs[i]\n",
    "            batch_x.append(sq_one_hot(test_id_sq[entry_id]))\n",
    "            \n",
    "        return np.array(batch_x).reshape(self.batch_size,TRIM_LENGTH,27,1)\n",
    "\n",
    "test_indexs = list(test_df.index)\n",
    "test_sq_generator = Test_Data_Generator(test_indexs,512)\n",
    "prediction = model.predict(test_sq_generator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d2a80d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df = []\n",
    "for indx,entry_id in enumerate(test_indexs):\n",
    "    for i in range(1500):\n",
    "        pred_df.append([entry_id,terms_mlb.classes_[i],prediction[indx][i]])\n",
    "    if indx%3000 == 0:\n",
    "        print(indx)\n",
    "\n",
    "submission = pd.DataFrame(pred_df)\n",
    "submission.columns = [\"EntryID\",\"Terms\",\"Prediction\"]\n",
    "submission.to_csv('submission.tsv', sep=\"\\t\",header = False,index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf] *",
   "language": "python",
   "name": "conda-env-tf-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
